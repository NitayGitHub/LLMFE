"""
[PREFIX]

###
<Task>
What is the current contraceptive method choice? no use, long-term methods, or short-term methods?

###
<Features>
[FEATURES]

###
<Examples>
[EXAMPLES]
[SUFFIX]
"""

@evaluate.run
def evaluate(data: dict):
    """ Evaluate the feature transformations on data observations."""
    
    from sklearn import preprocessing
    from sklearn.model_selection import StratifiedKFold, KFold
    from sklearn.metrics import accuracy_score
    from preprocessing import preprocess_datasets
    import xgboost as xgb
    import torch
    import numpy as np

    # Load data observations
    label_encoder = preprocessing.LabelEncoder()
    inputs, outputs, is_cat, is_regression = data['inputs'], data['outputs'], data['is_cat'], data['is_regression']
    X = modify_features(inputs)
    y = label_encoder.fit_transform(outputs)

    # Encode categorical string columns
    for col in X.columns:
        if X[col].dtype == 'string':
            X[col] = label_encoder.fit_transform(X[col])

    # Choose cross-validation strategy
    kf = KFold(n_splits=4, shuffle=True, random_state=42) if is_regression else StratifiedKFold(n_splits=4, shuffle=True, random_state=42)

    scores = []

    # 4-Fold Cross-Validation
    for train_idx, test_idx in kf.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        X_train_new, X_test_new = preprocess_datasets(X_train, X_test, None)
        
        # Convert to tensors
        X_train = torch.tensor(X_train_new.to_numpy())
        X_test = torch.tensor(X_test_new.to_numpy())

        # Load model
        model = xgb.XGBClassifier(random_state=42)

        model.fit(X_train_new, y_train)
        
        y_pred = model.predict(X_test_new)
        score = accuracy_score(y_test, y_pred)
        scores.append(score)

    # Calculate average score
    avg_score = np.mean(scores)

    return avg_score, inputs, outputs

@equation.evolve
def modify_features(df_input) -> pd.DataFrame:
    """
    Thought 1: Combining the education levels of the wife and husband provides a measure of the overall educational environment in the household. This could be useful in understanding how education influences contraceptive method choices.
    New Feature 1: Combined_Education_Index | Combined_Education_Index = (Wifes_education + Husbands_education) / 2
    """

    df_output = df_input.copy()
    # Calculate combined education index
    df_output['Combined_Education_Index'] = (df_output['Wifes_education'] + df_output['Husbands_education']) / 2

    return df_output